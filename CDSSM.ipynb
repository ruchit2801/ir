{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2671,
     "status": "ok",
     "timestamp": 1595753494068,
     "user": {
      "displayName": "2017 01070",
      "photoUrl": "",
      "userId": "14984631231630389713"
     },
     "user_tz": -330
    },
    "id": "qVgQhWCOst8E"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.util import ngrams\n",
    "import torch\n",
    "import gzip\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the dataloader \n",
    "\n",
    "This Dataset class helps to fetch the training data in mini-batches. The queries file is assumed to be in the format of dictionary `{k:v}`, in which, `k` is the query_id and `v` is the query sequence represented as a list of lists. Each list inside this list describes the words in the query. So, for the query of 10 words, we'd have a list of 10 lists. Each list stores the indices of letter trigrams present in the corresponding word.\n",
    "      \n",
    "### Example\n",
    "\n",
    "Let the query be `\"clean cities\"`. Since there are only two words, we'd have list of two list. Each of the two list can be described as : \n",
    "\n",
    "`clean cities` ===> `['clean', 'cities']` ===> `[['#cl', 'cle', 'lea', 'ean', 'an#'], ['#ci', 'cit', 'iti', 'tie', 'ies', 'es#']]`. We just replace the letter trigrams with their indices in the above list. \n",
    "\n",
    "We have the similar format of `doc_titles`. Variable `ds` representes the training data. I have assumed the pairwise training here. In pairwise training, we use (query, document) pairs for training. Each pair has query and its relevant document (the document that can answer the given query).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2669,
     "status": "ok",
     "timestamp": 1595753494071,
     "user": {
      "displayName": "2017 01070",
      "photoUrl": "",
      "userId": "14984631231630389713"
     },
     "user_tz": -330
    },
    "id": "lO2_VHaLsc6f"
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.queries = pickle.load(open('./DS/train_queries.p', 'rb'))\n",
    "        self.doc_titles = pickle.load(open('./DS/doc_titles.p', 'rb'))\n",
    "        \n",
    "        self.doc_map = list(self.doc_titles.keys())\n",
    "        self.coln_len = len(self.doc_map)\n",
    "        \n",
    "        self.ds = pickle.load(open('./P2-datasets/qrel_train.p', 'rb'))\n",
    "        self.ds = list(reversed(self.ds))\n",
    "        \n",
    "        # To cache the query tensors. This helps when we are considering top-k documents. e.g. k=5.\n",
    "        # This avoids the recalculation query_tensor for each of the five (Q, D) tuples. \n",
    "        self.local_cache = {}\n",
    "        self.init_negs_()\n",
    "        \n",
    "    # For negative docs, we periodically build the local database of 1000 randomly selected\n",
    "    # documents. By default, I rebuild the database each time I clean the cache. \n",
    "    def init_negs_(self):\n",
    "        idx = np.random.randint(0, self.coln_len, (1000,))\n",
    "        self.negs = torch.zeros(1000,30000, 10)\n",
    "        i_ = 0\n",
    "        for id_ in idx:\n",
    "            dseq = self.doc_titles[self.doc_map[id_]]\n",
    "            for i in range(1, min(9,len(dseq))+1):\n",
    "                self.negs[i_, dseq[i-1], i] = 1\n",
    "            i_ += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        qid = self.ds[index][0]\n",
    "        did = self.ds[index][1]\n",
    "        \n",
    "        qseq = self.queries[qid]\n",
    "        dseq = self.doc_titles[did]\n",
    "        \n",
    "        if qid in self.local_cache:\n",
    "            query_tensor = self.local_cache[qid]\n",
    "        else:\n",
    "            if(len(self.local_cache) > 1000):\n",
    "                self.local_cache = {}\n",
    "                self.init_negs_()\n",
    "                \n",
    "            query_tensor = torch.zeros(30000,20)\n",
    "            for i in range(1,len(qseq)+1):\n",
    "                query_tensor[qseq[i-1], i] = 1\n",
    "            self.local_cache[qid] = query_tensor\n",
    "            \n",
    "        pos_tensor = torch.zeros(30000, 20)\n",
    "        for i in range(1, len(dseq)+1):\n",
    "            pos_tensor[dseq[i-1],i] = 1\n",
    "        \n",
    "        idx = np.random.randint(0, len(self.negs), (4,))\n",
    "        neg_tensor = self.negs[idx]\n",
    "            \n",
    "        return query_tensor, pos_tensor, neg_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8449,
     "status": "ok",
     "timestamp": 1595753499857,
     "user": {
      "displayName": "2017 01070",
      "photoUrl": "",
      "userId": "14984631231630389713"
     },
     "user_tz": -330
    },
    "id": "-kRDIgZeAppc"
   },
   "outputs": [],
   "source": [
    "ds = Dataset()\n",
    "dl = DataLoader(ds, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDSSM model description\n",
    "\n",
    "CDSSM runs the 1-d convolution over the input sequence in order to extract the local contextual features from different parts of input sentence. Then the max-pooling layer builds the sentence level feature vector from the local contextual feature vectors. We then project this global sentence level feature map to a lower dimensional latent semantic space using the fully connected semantic layer. \n",
    "\n",
    "### 1-d Convulution : \n",
    "    We run 1-d convolution over the input sequence. Since the input sequence is sequence of letter tri-gram based word hashes, each word in the input sequence is represented as a 30000 dimensional vector based on letter-trigrams present in that word. We use a filter of length 3 here. Each convolutional operation produces the output vector of size 300, which represents the local feature map at different positions in the sentence. \n",
    "    \n",
    "### Max-pooling : \n",
    "\n",
    "    Max-pooling operation performs the max operation over the output of convolution layer over entire input sequence. We have fixed the query and document length to be 20 here, so, the max-pooling layer has fixed input size of 18 in our case. Likewise, the size of negative documents has been chosen to be 10, and so, the max pooling layer for negative docs is of input size 8. \n",
    "    \n",
    "### Semantic layer : \n",
    "\n",
    "    This is simple fully connected neural net to project the global feature map to latent semantic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8446,
     "status": "ok",
     "timestamp": 1595753499860,
     "user": {
      "displayName": "2017 01070",
      "photoUrl": "",
      "userId": "14984631231630389713"
     },
     "user_tz": -330
    },
    "id": "Qxe3JXIereB6"
   },
   "outputs": [],
   "source": [
    "class CDSSM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CDSSM, self).__init__()\n",
    "        \n",
    "        # Convolutional layer\n",
    "        self.conv = nn.Conv1d(30000, 300, 3, bias=False).cuda()\n",
    "        self.drop1 = nn.Dropout(0.15).cuda()\n",
    "        \n",
    "        # Max-pooling layer\n",
    "        self.max_pool = nn.MaxPool1d(18).cuda()\n",
    "        self.neg_max_pool = nn.MaxPool1d(8).cuda()\n",
    "        \n",
    "        # Semantic layer\n",
    "        self.sem = nn.Linear(300, 128, bias=False).cuda()\n",
    "        self.drop2 = nn.Dropout(0.15).cuda()\n",
    "        \n",
    "        # Temperature parameter of softmax function. \n",
    "        self.gamma = 10\n",
    "        \n",
    "    def forward(self, q, p, n):\n",
    "        \n",
    "        qc = self.conv(q)\n",
    "        qc_drop = self.drop1(qc)\n",
    "        qc_t = torch.tanh(qc_drop)\n",
    "        qc_max = self.max_pool(qc_t).squeeze(2)\n",
    "        qs = self.sem(qc_max)\n",
    "        qs_drop = self.drop2(qs)\n",
    "        qs_t = torch.tanh(qs_drop)\n",
    "        qs1 = qs_t.unsqueeze(1)\n",
    "        \n",
    "        pc = self.conv(p)\n",
    "        pc_drop = self.drop1(pc)\n",
    "        pc_t = torch.tanh(pc_drop)\n",
    "        pc_max = self.max_pool(pc_t).squeeze(2)\n",
    "        ps = self.sem(pc_max)\n",
    "        ps_drop = self.drop2(ps)\n",
    "        ps_t = torch.tanh(ps_drop)\n",
    "        ps1 = ps_t.unsqueeze(1)\n",
    "        \n",
    "        nc = self.conv(n)\n",
    "        nc_drop = self.drop1(nc)\n",
    "        nc_t = torch.tanh(nc_drop)\n",
    "        nc_mx = self.neg_max_pool(nc_t).squeeze(2)\n",
    "        ns = self.sem(nc_mx)\n",
    "        ns_drop = self.drop2(ns)\n",
    "        ns_t = torch.tanh(ns_drop)\n",
    "        ns1 = ns.view(len(q), 4, -1)\n",
    "        \n",
    "        ds = torch.cat((ps1, ns1), dim=1)\n",
    "        \n",
    "        R = torch.cosine_similarity(ds, qs1, dim=2)\n",
    "        with_gamma = R*gamma\n",
    "        return with_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12568,
     "status": "ok",
     "timestamp": 1595753503987,
     "user": {
      "displayName": "2017 01070",
      "photoUrl": "",
      "userId": "14984631231630389713"
     },
     "user_tz": -330
    },
    "id": "vf8pClyx6BCa"
   },
   "outputs": [],
   "source": [
    "model = CDSSM()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training : \n",
    "\n",
    "The following lines demonstrate the training of CDSSM. We can also fine tune the temperature parameter gamma based on validation set. Various values of gamma like [0.1, 10, 25, 40] should be tried. \n",
    "\n",
    "## After training : \n",
    "\n",
    "In my case, model was trained for about 25 epochs. We can also perform additional evaluaition of model on validation set at the end of each epoch. In fact, we also evaluated the model on validation set at each epoch. Once the mode has been trained, we can encode each document into latent semantic space using the trained model. This will give us a big tensor of size `[|C|, 128]`. Here `|C|` is the size of collection (number of documents in the corpus). `128` is the dimensionality of latent semantic space. The retrieval of relevant documents can be done based on cosine similarity between query vector (also encoded in the same space), and this huge collection tensor. We can then retrieve the top documents based on the cosine similarity scores. We were able to produce the MRR@10 of 0.215 on the MS-MARCO dataset using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "PATH = \"model_checkpoints/mod5e-4_\"\n",
    "\n",
    "mini_batch_losses = []\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y = torch.zeros((512, ), dtype=torch.long).cuda()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dl):\n",
    "        \n",
    "        q = batch[0].cuda()\n",
    "        p = batch[1].cuda()\n",
    "        \n",
    "        sh = list(batch[2].shape)\n",
    "        n = batch[2].view(sh[0]*sh[1], sh[2], sh[3]).cuda()\n",
    "        \n",
    "        if(len(y) != len(q)):\n",
    "            y = torch.zeros((len(q), ), dtype=torch.long).cuda()\n",
    "            \n",
    "        y_hat = model(q, p, n)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mini_batch_losses.append(loss.data)\n",
    "        epoch_loss += loss.data\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    \n",
    "    print(\"EPOCH : \", epoch, \" LOSS : \", tot_loss)\n",
    "    \n",
    "    model_path = PATH + str(epoch) + '.pt'\n",
    "    torch.save({'epoch' : epoch, \n",
    "                'model_state_dict': model.state_dict(), \n",
    "                'optimizer_state_dict': optimizer.state_dict(), \n",
    "                'loss': epoch_loss,\n",
    "                'mini_batch_losses' : mini_batch_losses,\n",
    "                'epoch_losses' : epoch_losses}, model_path)\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
