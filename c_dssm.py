import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import csv
import random
import gzip
from nltk.util import ngrams
import pickle

LETTER_GRAM_SIZE = 3
WINDOW_SIZE = 3
TOTAL_LETTER_GRAMS = 30000
WORD_DEPTH = WINDOW_SIZE * TOTAL_LETTER_GRAMS

K = 300
L = 128
J = 4
FILTER_LENGTH = 1

def kmax_pooling(x, dim, k):
    index = x.topk(k, dim=dim)[1].sort(dim=dim)[0]
    return x.gather(dim, index)

class CDSSM(nn.Module):
    def __init__(self):
        super(CDSSM, self).__init__()
        
        self.query_conv = nn.Conv1d(WORD_DEPTH, K, FILTER_LENGTH)
        self.query_sem = nn.Linear(K, L)
        
        self.doc_conv = nn.Conv1d(WORD_DEPTH, K, FILTER_LENGTH)
        self.query_sem = nn.Linear(K, L)
        self.learn_gamma = nn.Conv1d(1, 1, 1)
        
    def forward(self, q, pos, negs):
        """
        In this step, we transform each word vector with WORD_DEPTH dimensions into its
        convolved representation with K dimensions. K is the number of kernels/filters
        being used in the operation. Essentially, the operation is taking the dot product
        of a single weight matrix (W_c) with each of the word vectors (l_t) from the
        query matrix (l_Q), adding a bias vector (b_c), and then applying the tanh activation.
        That is, h_Q = tanh(W_c â€¢ l_Q + b_c). Note: the paper does not include bias units.
        """
    
        q = q.transpose(1,2)
        
        q_c = F.tanh(self.query_conv(q))
        q_k = kmax_pooling(q_c, 2, 1)
        q_k = q_k.transpose(1, 2)
        
        q_s = F.tanh(self.query_sem(q_k))
        q_s = q_s.resize(L)
        
        pos = pos.transpose(1,2)
        pos_c = F.tanh(self.doc_conv(pos))
        pos_k = kmax_pooling(pos_c, 2, 1)
        pos_k = pos_k.transpose(1,2)
        pos_s = F.tanh(self.doc_sem(pos_k))
        pos_s = pos_s.resize(L)


        negs = [neg.transpose(1,2) for neg in negs]
        neg_cs = [F.tanh(self.doc_conv(neg)) for neg in negs]
        neg_ks = [kmax_pooling(neg_c, 2, 1) for neg_c in neg_cs]
        neg_ks = [neg_k.transpose(1,2) for neg_k in neg_ks]
        neg_ss = [F.tanh(self.doc_sem(neg_k)) for neg_k in neg_ks]
        neg_ss = [neg_s.resize(L) for neg_s in neg_ss]


        dots = [q_s.dot(pos_s)]
        dots = dots + [q_s.dot(neg_s) for neg_s in neg_ss]
        dots = torch.stack(dots)


        with_gamma = self.learn_gamma(dots.resize(J+1, 1, 1))
        return with_gamma
        
model = CDSSM()


# Loss and optimizer
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)

# Preorocessing goes from here
def getcontent(docid, f):
    """getcontent(docid, f) will get content for a given docid (a string) from filehandle f.
    The content has four tab-separated strings: docid, url, title, body.
    """

    f.seek(docoffset[docid])
    line = f.readline()
    assert line.startswith(docid + "\t"), \
        f"Looking for {docid}, found {line}"
    return line.rstrip() 

# The query string for each topicid is querystring[topicid]
querystring = {}
with gzip.open("msmarco-doctrain-queries.tsv.gz", 'rt', encoding='utf8') as f:
    tsvreader = csv.reader(f, delimiter="\t")
    for [topicid, querystring_of_topicid] in tsvreader:
        querystring[topicid] = querystring_of_topicid

# For each topicid, the list of positive docids is qrel[topicid]
qrel = {}
with gzip.open("msmarco-doctrain-qrels.tsv.gz", 'rt', encoding='utf8') as f:
    tsvreader = csv.reader(f, delimiter="\t")
    for [topicid, _, docid, rel] in tsvreader:
        assert rel == "1"
        if topicid in qrel:
            qrel[topicid].append(docid)
        else:
            qrel[topicid] = [docid]
            
docoffset = {}
with gzip.open("msmarco-docs-lookup.tsv.gz", 'rt', encoding='utf8') as f:
    tsvreader = csv.reader(f, delimiter="\t")
    for [docid, _, offset] in tsvreader:
        docoffset[docid] = int(offset)
        # if(text_to_hot_vector(text[i+2])==[]):
        #     continue
f = open('indexed_lookup_table.pickle', 'rb')
indexed_lookup_table = pickle.load(f)

i=0

def hash_token(token):
    """
    Indexed lookup table is top 30k letter trigrams occuring in the corpus.
    It is generated by processing MS-MARCO docs dataset. For input parameter
    token, this function first splits the token in letter trigrams, and then, 
    it hashes every token in the same way desctibed in the paper. 
    """
    trigrams = list(map(lambda x: ''.join(x), ngrams(token, 3)))
    hashed_token = np.zeros(30000)
    flag = 1
    for trigram in trigrams:
        if trigram in indexed_lookup_table:
            hashed_token[indexed_lookup_table[trigram]]+=1
            flag = 0
    if flag:
        return None
    return hashed_token
    
    
def generate_seq(text):
    """ 
    This function generates the sequence of 90k sized vectors each representing
    word trigram of input parameter text. This sequence can be fed directly into 
    the model for training. 
    """
    text = text.lower()
    tokens = text.split()
    word_seq = []
    for token in tokens:
        hashed = hash_token(token)
        if hashed is not None:
            word_seq.append(hashed)
    if word_seq == []:
        return None
    word_trigrams = np.array(list(map(lambda x: np.concatenate(x), ngrams(word_seq, 3))))
    return word_trigrams


# Check if GPU is available to speed up the execution
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    print("Running on the GPU")
else:
    device = torch.device("cpu")
    print("Running on the CPU")


# For every query-doc pair in qrels, we train our model, by taking into account
# the relevant document to that query, and four negative documents randomly selected
# from the list of documents. 
with open("msmarco-docs.tsv", encoding="utf8") as f:
    i=0
    qlen=0
    dlen=0
    for key in qrel:
        query ='<s> ' +  querystring[key] + ' <s>'
        pos = '<s> '  + getcontent(qrel[key][0], f) + ' <s>'
        neg_docs = random.sample(docoffset.keys(), 4)
        while key in neg_docs:
            neg_docs = random.sample(docoffset.keys(), 4)
        neg_docs = ['<s> ' + getcontent(docid, f) + ' <s>' for docid in neg_docs]
        
        query_seq = generate_seq(query)
        pos_seq = generate_seq(pos)
        neg_seqs = [generate_seq(doc) for doc in neg_docs]
        
        if query_seq is None:
            continue
        
        query_seq = query_seq.reshape(1, len(query_seq), WORD_DEPTH)
        pos_seq = pos_seq.reshape(1, len(pos_seq), WORD_DEPTH)
        neg_seqs = [seq.reshape(1, len(seq), WORD_DEPTH) for seq in neg_seqs]
        
        query = Variable(torch.from_numpy(query_seq).float())
        pos = Variable(torch.from_numpy(pos_seq).float())
        negs = []
        for j in range(J):
            negs.append(Variable(torch.from_numpy(neg_seqs[j]).float()))
            
            
        # Transfer the tensors on the GPU
        query = query.to(device)
        pos = pos.to(device)
        negs = negs.to(device)
        
        y = np.ndarray(1)
        y[0]=0
        y = Variable(torch.from_numpy(y).long())
    
        y_pred = model(query, pos, neg_docs)
        loss = criterion(y_pred.resize(1,J+1), y)
        print (i, loss.data.item())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        i+=1
        break
            
